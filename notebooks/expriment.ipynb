{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e836736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb2d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classification\\classification_pipeline\\app\\Dataset\\Cancer\\30872385.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32a9e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ID:30872385>',\n",
       " 'Title: Comparison of methodologies for the detection of BRAF mutations in bone marrow trephine specimens.',\n",
       " \"Abstract: AIMS: BRAF V600E detection assists in the diagnosis of hairy cell leukaemia (HCL); however, testing practices vary. We evaluated the clinical utility of 5 BRAF mutation testing strategies for use on bone marrow trephines (BMT). METHODS: 11 HCL, 5 HCL 'mimic', 2 treated HCL and 10 normal BMT specimens were tested for mutant BRAF, comparing Sanger sequencing, pyrosequencing, amplicon-based next generation sequencing (NGS), automated (Idylla) PCR and immunohistochemistry (IHC). RESULTS: PCR and IHC were cheaper and identified V600E in 100 % of HCL cases. Pyrosequencing detected the mutation in 91%, NGS in 55% of cases and Sanger sequencing in 27%. All assays gave wild-type BRAF results in HCL mimics and normal BMT samples. CONCLUSIONS: PCR and IHC were most sensitive and cost-effective, but these have limited scope for multiplexing and are likely to be replaced by NGS gene panels or whole genome sequencing in the medium to long term.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.strip().split(\"\\n\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88178635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID                                              Title  \\\n",
      "0  30872385  Comparison of methodologies for the detection ...   \n",
      "\n",
      "                                            Abstract  \n",
      "0  AIMS: BRAF V600E detection assists in the diag...  \n"
     ]
    }
   ],
   "source": [
    "# Parse text into components\n",
    "lines = text.strip().split('\\n')\n",
    "\n",
    "data = {\n",
    "    'ID': lines[0].replace('<ID:', '').replace('>', '').strip(),\n",
    "    'Title': lines[1].replace('Title:', '').strip(),\n",
    "    'Abstract': lines[2].replace('Abstract:', '').strip()\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([data])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00df062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30872385&gt;\\nTitle: Comparison of methodolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30873683&gt;\\nTitle: Tumour biomarkers-Tracin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30874851&gt;\\nTitle: Pomalidomide, cyclophosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875581&gt;\\nTitle: Aggressive variants of p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875950&gt;\\nTitle: Circulating Tumour Cells...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path   label  \\\n",
       "0  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Cancer   \n",
       "1  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Cancer   \n",
       "2  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Cancer   \n",
       "3  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Cancer   \n",
       "4  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Cancer   \n",
       "\n",
       "                                             content  \n",
       "0  <ID:30872385>\\nTitle: Comparison of methodolog...  \n",
       "1  <ID:30873683>\\nTitle: Tumour biomarkers-Tracin...  \n",
       "2  <ID:30874851>\\nTitle: Pomalidomide, cyclophosp...  \n",
       "3  <ID:30875581>\\nTitle: Aggressive variants of p...  \n",
       "4  <ID:30875950>\\nTitle: Circulating Tumour Cells...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#base directory\n",
    "base_dir = r\"C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classification\\classification_pipeline\\app\\Dataset\"\n",
    "\n",
    "#data colection\n",
    "data = []\n",
    "\n",
    "#using loop \"cancer\", \"uncancer\"\n",
    "for label in [\"Cancer\", \"Non-Cancer\"]:\n",
    "    folder_path = os.path.join(base_dir, label)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            #print(file_path)\n",
    "            \n",
    "            #read the text file \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                \n",
    "            data.append({\n",
    "                \"file_path\":file_path,\n",
    "                \"label\": label,\n",
    "                \"content\": content\n",
    "            })\n",
    "#create dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787539f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>ids</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30872385&gt;\\nTitle: Comparison of methodolog...</td>\n",
       "      <td>30872385</td>\n",
       "      <td>Comparison of methodologies for the detection ...</td>\n",
       "      <td>AIMS: BRAF V600E detection assists in the diag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30873683&gt;\\nTitle: Tumour biomarkers-Tracin...</td>\n",
       "      <td>30873683</td>\n",
       "      <td>Tumour biomarkers-Tracing the molecular functi...</td>\n",
       "      <td>In recent years, with the increase in cancer m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30874851&gt;\\nTitle: Pomalidomide, cyclophosp...</td>\n",
       "      <td>30874851</td>\n",
       "      <td>Pomalidomide, cyclophosphamide, and dexamethas...</td>\n",
       "      <td>Pomalidomide dexamethasone is a standard of ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875581&gt;\\nTitle: Aggressive variants of p...</td>\n",
       "      <td>30875581</td>\n",
       "      <td>Aggressive variants of prostate cancer - Are w...</td>\n",
       "      <td>Recently, adoption of novel drugs for systemic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875950&gt;\\nTitle: Circulating Tumour Cells...</td>\n",
       "      <td>30875950</td>\n",
       "      <td>Circulating Tumour Cells (CTC), Head and Neck ...</td>\n",
       "      <td>Head and neck cancer is the seventh most commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38623902&gt;\\nTitle: [Not Available].\\nAbstra...</td>\n",
       "      <td>38623902</td>\n",
       "      <td>[Not Available].</td>\n",
       "      <td>Effective longitudinal biomarkers that track d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38640937&gt;\\nTitle: Mechanisms and managemen...</td>\n",
       "      <td>38640937</td>\n",
       "      <td>Mechanisms and management of loss of response ...</td>\n",
       "      <td>We sought to report the effectiveness of infli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38642556&gt;\\nTitle: Modification of coronary...</td>\n",
       "      <td>38642556</td>\n",
       "      <td>Modification of coronary artery disease clinic...</td>\n",
       "      <td>The extent to which the relationships between ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38650020&gt;\\nTitle: Meta-analysis of the glo...</td>\n",
       "      <td>38650020</td>\n",
       "      <td>Meta-analysis of the global distribution of cl...</td>\n",
       "      <td>CYP2C8 is responsible for the metabolism of 5%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38701783&gt;\\nTitle: FLT3L governs the develo...</td>\n",
       "      <td>38701783</td>\n",
       "      <td>FLT3L governs the development of partially ove...</td>\n",
       "      <td>FMS-related tyrosine kinase 3 ligand (FLT3L), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_path       label  \\\n",
       "0    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "1    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "2    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "3    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "4    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "..                                                 ...         ...   \n",
       "995  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "996  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "997  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "998  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "999  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "\n",
       "                                               content       ids  \\\n",
       "0    <ID:30872385>\\nTitle: Comparison of methodolog...  30872385   \n",
       "1    <ID:30873683>\\nTitle: Tumour biomarkers-Tracin...  30873683   \n",
       "2    <ID:30874851>\\nTitle: Pomalidomide, cyclophosp...  30874851   \n",
       "3    <ID:30875581>\\nTitle: Aggressive variants of p...  30875581   \n",
       "4    <ID:30875950>\\nTitle: Circulating Tumour Cells...  30875950   \n",
       "..                                                 ...       ...   \n",
       "995  <ID:38623902>\\nTitle: [Not Available].\\nAbstra...  38623902   \n",
       "996  <ID:38640937>\\nTitle: Mechanisms and managemen...  38640937   \n",
       "997  <ID:38642556>\\nTitle: Modification of coronary...  38642556   \n",
       "998  <ID:38650020>\\nTitle: Meta-analysis of the glo...  38650020   \n",
       "999  <ID:38701783>\\nTitle: FLT3L governs the develo...  38701783   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Comparison of methodologies for the detection ...   \n",
       "1    Tumour biomarkers-Tracing the molecular functi...   \n",
       "2    Pomalidomide, cyclophosphamide, and dexamethas...   \n",
       "3    Aggressive variants of prostate cancer - Are w...   \n",
       "4    Circulating Tumour Cells (CTC), Head and Neck ...   \n",
       "..                                                 ...   \n",
       "995                                   [Not Available].   \n",
       "996  Mechanisms and management of loss of response ...   \n",
       "997  Modification of coronary artery disease clinic...   \n",
       "998  Meta-analysis of the global distribution of cl...   \n",
       "999  FLT3L governs the development of partially ove...   \n",
       "\n",
       "                                              abstract  \n",
       "0    AIMS: BRAF V600E detection assists in the diag...  \n",
       "1    In recent years, with the increase in cancer m...  \n",
       "2    Pomalidomide dexamethasone is a standard of ca...  \n",
       "3    Recently, adoption of novel drugs for systemic...  \n",
       "4    Head and neck cancer is the seventh most commo...  \n",
       "..                                                 ...  \n",
       "995  Effective longitudinal biomarkers that track d...  \n",
       "996  We sought to report the effectiveness of infli...  \n",
       "997  The extent to which the relationships between ...  \n",
       "998  CYP2C8 is responsible for the metabolism of 5%...  \n",
       "999  FMS-related tyrosine kinase 3 ligand (FLT3L), ...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data parse from content\n",
    "data1 = []\n",
    "def data_parse(df):\n",
    "    content_list = [i for i in df[\"content\"]]\n",
    "    for text in content_list:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        id_ = lines[0].replace('<ID:', '').replace('>', '').strip()\n",
    "        title = lines[1].replace('Title:', '').strip()\n",
    "        abstract = lines[2].replace('Abstract:', '').strip()\n",
    "        #print(id_, title, abstract)\n",
    "        data1.append({\n",
    "            # 'file_path':df[\"file_path\"],\n",
    "            # 'label': df[\"label\"][0],\n",
    "            \"ids\":id_,\n",
    "            \"title\":title,\n",
    "            \"abstract\":abstract\n",
    "        })\n",
    "data_parse(df)\n",
    "\n",
    "df_ = pd.DataFrame(data1)\n",
    "df[\"ids\"] = df_[\"ids\"]\n",
    "df[\"title\"] = df_[\"title\"]\n",
    "df[\"abstract\"] =df_[\"abstract\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab692ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#define cleaning function\n",
    "def clean_text(text):\n",
    "    #changing into Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    #remove special characters & punctuation.\n",
    "    #text = re.sub(rf'[{re.escape(string.punctuation)}]', '', text)\n",
    "\n",
    "    #remove numbers\n",
    "    #text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    #remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #Lemmatization (without removing stopwords)\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    #Joining back to string\n",
    "    return ' '.join(cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500670cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = df[\"title\"].apply(clean_text)\n",
    "df[\"abstract\"] = df[\"abstract\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b181b864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>ids</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30872385&gt;\\nTitle: Comparison of methodolog...</td>\n",
       "      <td>30872385</td>\n",
       "      <td>comparison of methodology for the detection of...</td>\n",
       "      <td>aim : braf v600e detection assist in the diagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30873683&gt;\\nTitle: Tumour biomarkers-Tracin...</td>\n",
       "      <td>30873683</td>\n",
       "      <td>tumour biomarkers-tracing the molecular functi...</td>\n",
       "      <td>in recent year , with the increase in cancer m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30874851&gt;\\nTitle: Pomalidomide, cyclophosp...</td>\n",
       "      <td>30874851</td>\n",
       "      <td>pomalidomide , cyclophosphamide , and dexameth...</td>\n",
       "      <td>pomalidomide dexamethasone is a standard of ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875581&gt;\\nTitle: Aggressive variants of p...</td>\n",
       "      <td>30875581</td>\n",
       "      <td>aggressive variant of prostate cancer - are we...</td>\n",
       "      <td>recently , adoption of novel drug for systemic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875950&gt;\\nTitle: Circulating Tumour Cells...</td>\n",
       "      <td>30875950</td>\n",
       "      <td>circulating tumour cell ( ctc ) , head and nec...</td>\n",
       "      <td>head and neck cancer is the seventh most commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38623902&gt;\\nTitle: [Not Available].\\nAbstra...</td>\n",
       "      <td>38623902</td>\n",
       "      <td>[ not available ] .</td>\n",
       "      <td>effective longitudinal biomarkers that track d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38640937&gt;\\nTitle: Mechanisms and managemen...</td>\n",
       "      <td>38640937</td>\n",
       "      <td>mechanism and management of loss of response t...</td>\n",
       "      <td>we sought to report the effectiveness of infli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38642556&gt;\\nTitle: Modification of coronary...</td>\n",
       "      <td>38642556</td>\n",
       "      <td>modification of coronary artery disease clinic...</td>\n",
       "      <td>the extent to which the relationship between c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38650020&gt;\\nTitle: Meta-analysis of the glo...</td>\n",
       "      <td>38650020</td>\n",
       "      <td>meta-analysis of the global distribution of cl...</td>\n",
       "      <td>cyp2c8 is responsible for the metabolism of 5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38701783&gt;\\nTitle: FLT3L governs the develo...</td>\n",
       "      <td>38701783</td>\n",
       "      <td>flt3l governs the development of partially ove...</td>\n",
       "      <td>fms-related tyrosine kinase 3 ligand ( flt3l )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_path       label  \\\n",
       "0    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "1    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "2    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "3    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "4    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "..                                                 ...         ...   \n",
       "995  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "996  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "997  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "998  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "999  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "\n",
       "                                               content       ids  \\\n",
       "0    <ID:30872385>\\nTitle: Comparison of methodolog...  30872385   \n",
       "1    <ID:30873683>\\nTitle: Tumour biomarkers-Tracin...  30873683   \n",
       "2    <ID:30874851>\\nTitle: Pomalidomide, cyclophosp...  30874851   \n",
       "3    <ID:30875581>\\nTitle: Aggressive variants of p...  30875581   \n",
       "4    <ID:30875950>\\nTitle: Circulating Tumour Cells...  30875950   \n",
       "..                                                 ...       ...   \n",
       "995  <ID:38623902>\\nTitle: [Not Available].\\nAbstra...  38623902   \n",
       "996  <ID:38640937>\\nTitle: Mechanisms and managemen...  38640937   \n",
       "997  <ID:38642556>\\nTitle: Modification of coronary...  38642556   \n",
       "998  <ID:38650020>\\nTitle: Meta-analysis of the glo...  38650020   \n",
       "999  <ID:38701783>\\nTitle: FLT3L governs the develo...  38701783   \n",
       "\n",
       "                                                 title  \\\n",
       "0    comparison of methodology for the detection of...   \n",
       "1    tumour biomarkers-tracing the molecular functi...   \n",
       "2    pomalidomide , cyclophosphamide , and dexameth...   \n",
       "3    aggressive variant of prostate cancer - are we...   \n",
       "4    circulating tumour cell ( ctc ) , head and nec...   \n",
       "..                                                 ...   \n",
       "995                                [ not available ] .   \n",
       "996  mechanism and management of loss of response t...   \n",
       "997  modification of coronary artery disease clinic...   \n",
       "998  meta-analysis of the global distribution of cl...   \n",
       "999  flt3l governs the development of partially ove...   \n",
       "\n",
       "                                              abstract  \n",
       "0    aim : braf v600e detection assist in the diagn...  \n",
       "1    in recent year , with the increase in cancer m...  \n",
       "2    pomalidomide dexamethasone is a standard of ca...  \n",
       "3    recently , adoption of novel drug for systemic...  \n",
       "4    head and neck cancer is the seventh most commo...  \n",
       "..                                                 ...  \n",
       "995  effective longitudinal biomarkers that track d...  \n",
       "996  we sought to report the effectiveness of infli...  \n",
       "997  the extent to which the relationship between c...  \n",
       "998  cyp2c8 is responsible for the metabolism of 5 ...  \n",
       "999  fms-related tyrosine kinase 3 ligand ( flt3l )...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba03ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_loader.py\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Base directory\n",
    "# BASE_DIR = r\"C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classification\\classification_pipeline\\app\\Dataset\"\n",
    "\n",
    "# # Initialize list for storing data\n",
    "# data = []\n",
    "\n",
    "# # Loop through Cancer and Non-Cancer folders\n",
    "# for label in [\"Cancer\", \"Non-Cancer\"]:\n",
    "#     folder_path = os.path.join(BASE_DIR, label)\n",
    "\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         if filename.endswith(\".txt\"):\n",
    "#             file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "#             # Read the content\n",
    "#             with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 content = f.read().strip()\n",
    "\n",
    "#             # Parse ID, Title, Abstract\n",
    "#             try:\n",
    "#                 lines = content.split(\"\\n\")\n",
    "#                 id_ = lines[0].replace('<ID:', '').replace('>', '').strip()\n",
    "#                 title = lines[1].replace('Title:', '').strip()\n",
    "#                 abstract = lines[2].replace('Abstract:', '').strip()\n",
    "#             except IndexError:\n",
    "#                 print(f\"Malformed file skipped: {file_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Append parsed data\n",
    "#             data.append({\n",
    "#                 \"file_path\": file_path,\n",
    "#                 \"label\": label,\n",
    "#                 \"ids\": id_,\n",
    "#                 \"title\": title,\n",
    "#                 \"abstract\": abstract,\n",
    "#                 \"content\": content\n",
    "#             })\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Optional: show a sample\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff8f5a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "..  ..\n",
       "995  0\n",
       "996  0\n",
       "997  0\n",
       "998  0\n",
       "999  0\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label= np.load(r\"C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classification\\classification_pipeline\\app\\labels.npy\")\n",
    "\n",
    "import pandas as pd \n",
    "pd.DataFrame(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37498f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05865531, -0.06116756, -0.04841614, ..., -0.13194865,\n",
       "         0.00626522, -0.09212897],\n",
       "       [-0.04267618, -0.02818019, -0.0572282 , ..., -0.01069069,\n",
       "         0.01573799, -0.01285062],\n",
       "       [ 0.00647902, -0.01608148, -0.03012745, ..., -0.07893753,\n",
       "        -0.00581685, -0.09062544],\n",
       "       ...,\n",
       "       [ 0.02162201,  0.03952667,  0.00819065, ..., -0.02779166,\n",
       "         0.09780525,  0.01256192],\n",
       "       [-0.01750086, -0.00029801,  0.05761583, ...,  0.04505253,\n",
       "         0.01330023, -0.06579966],\n",
       "       [-0.09487092, -0.00049002, -0.04533917, ..., -0.04889944,\n",
       "         0.01735684,  0.04944939]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd = np.load(r\"C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classification\\classification_pipeline\\app\\embeddings.npy\")\n",
    "embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.86553067e-02, -6.11675642e-02, -4.84161414e-02, -1.29827574e-01,\n",
       "        -7.55321905e-02, -3.89920287e-02, -7.48163536e-02,  9.17119682e-02,\n",
       "         2.15303637e-02,  3.15023996e-02, -2.02053525e-02, -2.61817481e-02,\n",
       "         1.78138334e-02,  3.38815376e-02, -1.22108355e-01,  8.25391989e-03,\n",
       "        -7.03520281e-03, -1.60418320e-02,  1.35447038e-02, -9.78858545e-02,\n",
       "        -1.07826211e-01,  4.66004834e-02,  4.39636596e-02,  6.78262860e-02,\n",
       "        -1.95260171e-03, -1.13929935e-01, -2.94170505e-03,  3.55189666e-02,\n",
       "        -1.19355889e-02,  2.35840604e-02,  1.58432275e-02,  8.45380127e-02,\n",
       "         8.95186290e-02, -2.03057062e-02,  1.68474652e-02,  4.90528010e-02,\n",
       "        -5.40754981e-02,  1.16312327e-02,  3.99037525e-02, -1.80040039e-02,\n",
       "         5.38776591e-02, -1.23859905e-01, -7.24078640e-02,  4.48571108e-02,\n",
       "         2.87048910e-02, -6.24615848e-02, -5.72517747e-03,  3.26595060e-03,\n",
       "        -4.53652767e-03,  1.38715804e-01,  2.08635256e-02, -1.18058771e-02,\n",
       "        -1.46961666e-03, -5.31868497e-03, -6.52265251e-02, -1.22318871e-01,\n",
       "         3.35992053e-02,  6.73866319e-03,  5.07671274e-02,  8.95300731e-02,\n",
       "        -1.14602139e-02, -2.70644464e-02, -1.88062675e-02, -1.54408263e-02,\n",
       "         1.13487793e-02,  1.74860284e-02,  8.44270885e-02, -3.92919667e-02,\n",
       "         3.64967287e-02, -6.91259801e-02,  4.67049703e-02, -1.46202110e-02,\n",
       "         1.51368240e-02,  4.73404191e-02, -7.83391595e-02,  3.52430679e-02,\n",
       "         7.02226534e-02, -2.29925942e-02, -6.60126209e-02,  5.85085340e-03,\n",
       "        -7.44481832e-02,  3.63611542e-02,  5.69302738e-02, -3.88094299e-02,\n",
       "         1.97543204e-02, -1.82198510e-02,  2.54389141e-02,  4.57240827e-02,\n",
       "        -1.11283772e-01, -9.62516479e-03,  9.68801528e-02,  5.34671694e-02,\n",
       "        -1.29510614e-03, -1.81156974e-02,  3.23460251e-02,  4.60971370e-02,\n",
       "         4.57614437e-02,  6.11816794e-02,  1.15625896e-01, -4.50045280e-02,\n",
       "        -4.16326970e-02, -4.28020805e-02,  2.77853962e-02,  9.79623571e-03,\n",
       "        -7.49721471e-03, -5.66920452e-02, -1.10084536e-02, -6.09574616e-02,\n",
       "        -5.84948156e-03, -9.50468853e-02, -1.45518605e-03,  4.31456529e-02,\n",
       "         5.52422665e-02,  1.50601240e-02,  1.03901708e-02, -5.27432300e-02,\n",
       "        -2.67737769e-02, -1.01008406e-02,  6.25522137e-02, -1.71172917e-02,\n",
       "         4.69689965e-02, -1.68455020e-02,  3.41262855e-02,  2.61481088e-02,\n",
       "         9.41702947e-02, -2.34353752e-03,  3.53294984e-02,  6.98580350e-33,\n",
       "        -7.93870259e-03,  1.19316787e-01,  5.26759811e-02,  2.21528132e-02,\n",
       "        -1.20368779e-01,  2.32079625e-02,  2.06534215e-03, -2.01006215e-02,\n",
       "        -7.82888532e-02,  4.97715920e-02,  5.61837060e-03, -1.89283714e-02,\n",
       "         8.62922147e-02, -2.35164706e-02, -3.31536606e-02, -1.78443640e-02,\n",
       "        -3.53383273e-02, -2.54658479e-02, -8.44430476e-02,  2.40613502e-02,\n",
       "         1.80259869e-02, -1.33494390e-02, -2.04686727e-02, -8.62432271e-02,\n",
       "        -5.22118881e-02,  3.35628204e-02,  1.33930026e-02, -7.91672170e-02,\n",
       "         6.18629567e-02, -2.50648372e-02,  2.81004645e-02,  2.69946642e-03,\n",
       "         4.58870605e-02,  2.83453614e-02, -2.27829162e-02, -2.23632175e-02,\n",
       "         6.13575149e-03,  1.31243756e-02, -8.64602532e-03,  5.03658354e-02,\n",
       "         1.58561170e-02,  1.31277945e-02,  3.19983661e-02, -3.70476730e-02,\n",
       "         9.20635611e-02, -9.53923017e-02, -1.79565214e-02, -4.03143503e-02,\n",
       "        -8.75313282e-02, -1.83280092e-02, -6.60718931e-03, -1.66395176e-02,\n",
       "         2.52480842e-02,  1.95959453e-02,  2.51955967e-02, -4.03442979e-02,\n",
       "        -6.20769970e-02, -4.94552664e-02,  6.70571625e-02,  1.08583122e-01,\n",
       "         3.93404439e-02, -3.47314291e-02,  1.33930158e-03,  4.67030369e-02,\n",
       "         3.05400789e-02, -9.83934104e-02, -4.50815773e-03, -2.49079634e-02,\n",
       "        -4.75203209e-02,  9.75600481e-02,  7.60221630e-02, -2.33472772e-02,\n",
       "         1.80309638e-02, -3.49000394e-02, -9.12822597e-03,  9.18523502e-03,\n",
       "        -3.38787585e-02,  1.18618041e-01, -3.61656733e-02, -1.05614578e-02,\n",
       "        -2.26026885e-02,  3.84585187e-02, -2.38301512e-02,  6.02528416e-02,\n",
       "        -4.13064063e-02, -6.87128901e-02,  8.65611993e-03, -2.36717016e-02,\n",
       "        -3.00135501e-02,  2.99781449e-02,  1.09145297e-02, -1.22881150e-02,\n",
       "        -5.89618646e-02, -3.90607119e-02,  5.94421104e-02, -6.99699347e-33,\n",
       "         1.11013070e-01,  7.22966250e-03,  7.17364773e-02, -3.93752269e-02,\n",
       "        -5.25953211e-02, -1.43617969e-02,  7.42565021e-02, -2.20821705e-02,\n",
       "         3.21453214e-02, -1.19907567e-02,  1.53301284e-01,  3.59485298e-02,\n",
       "        -6.37626722e-02, -4.00768407e-02, -9.86028388e-02,  2.57759430e-02,\n",
       "        -2.66781934e-02, -7.57727819e-03, -9.70151797e-02,  4.49091978e-02,\n",
       "        -1.27048027e-02,  6.43796027e-02, -3.35915349e-02, -1.77349187e-02,\n",
       "        -1.59958992e-02,  2.71781702e-02,  1.10811996e-03, -5.51637681e-03,\n",
       "         6.94385022e-02, -1.16258347e-02, -2.83906050e-02,  9.46840718e-02,\n",
       "         7.71084800e-03, -1.62648980e-03,  5.75411581e-02,  1.08414609e-02,\n",
       "         8.90564919e-02,  1.38350818e-02, -1.25274928e-02, -9.72034317e-03,\n",
       "        -3.37951928e-02,  7.94165656e-02, -2.14213487e-02, -2.02491544e-02,\n",
       "         5.76952994e-02,  3.97817530e-02, -4.18276433e-03,  2.99653616e-02,\n",
       "         9.35091302e-02,  1.31338928e-02, -1.07165764e-03,  4.60926369e-02,\n",
       "        -1.65761840e-02,  2.51259971e-02,  4.24254779e-03, -7.07660019e-02,\n",
       "        -1.04425447e-02, -8.30176994e-02, -6.16468973e-02, -4.02253866e-02,\n",
       "         3.77814919e-02,  2.00592615e-02,  2.42597368e-02,  9.38399881e-03,\n",
       "        -1.49914585e-02,  4.80639972e-02, -1.92012303e-02, -6.21112436e-03,\n",
       "         1.65293142e-02, -2.44223904e-02, -7.40769953e-02, -1.27500640e-02,\n",
       "         6.62379861e-02, -7.10893199e-02,  2.67712772e-03, -8.49206001e-02,\n",
       "        -5.73282540e-02, -2.70532593e-02,  1.85750294e-02,  1.86167993e-02,\n",
       "        -1.86278264e-03, -6.20002002e-02, -1.61714721e-02, -1.11690387e-02,\n",
       "        -2.83853710e-03,  1.81022333e-03,  1.46030933e-02,  2.26606568e-03,\n",
       "        -5.42185418e-02, -1.17690675e-01, -6.02762215e-02, -4.42821383e-02,\n",
       "         1.52561432e-02, -7.37952208e-03,  3.98277566e-02, -5.06578672e-08,\n",
       "         9.13222209e-02, -5.57759106e-02, -2.18596049e-02, -1.01231806e-01,\n",
       "        -3.35046649e-02,  3.57586704e-02, -9.62085724e-02,  1.97911132e-02,\n",
       "         1.70746986e-02,  2.10553352e-02,  1.25877075e-02,  5.98562043e-03,\n",
       "        -5.21565080e-02,  7.60728493e-03,  1.55221019e-02,  2.16783974e-02,\n",
       "         2.35752575e-02, -7.30812326e-02,  3.90048660e-02, -2.40557399e-02,\n",
       "        -2.71937437e-02,  3.83126251e-02,  1.69600677e-02, -1.81615166e-02,\n",
       "         2.32561100e-02,  5.66040128e-02,  6.68604858e-03,  2.04847287e-02,\n",
       "         6.16980456e-02, -6.50954917e-02,  6.00101650e-02, -1.83652695e-02,\n",
       "         1.95142310e-02,  2.35148966e-02,  8.57656002e-02,  7.40364864e-02,\n",
       "        -2.80872919e-02,  5.15213385e-02,  4.94793318e-02,  3.43825817e-02,\n",
       "         1.40681835e-02, -4.74031568e-02, -1.32946566e-01, -1.33857680e-02,\n",
       "        -1.36661595e-02, -6.75664693e-02,  1.43807288e-03, -4.38051969e-02,\n",
       "         2.21227184e-02, -8.19865242e-02, -3.93938869e-02, -2.06733141e-02,\n",
       "         2.85880547e-02,  6.75816264e-04,  4.96700071e-02,  1.41361535e-01,\n",
       "         4.33737673e-02, -1.76472478e-02,  7.78694749e-02,  4.66249771e-02,\n",
       "         6.12840503e-02, -1.31948650e-01,  6.26522256e-03, -9.21289697e-02],\n",
       "       dtype=float32),\n",
       " 384)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd[0], len(embd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b674e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>ids</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30872385&gt;\\nTitle: Comparison of methodolog...</td>\n",
       "      <td>30872385</td>\n",
       "      <td>comparison of methodology for the detection of...</td>\n",
       "      <td>aim : braf v600e detection assist in the diagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30873683&gt;\\nTitle: Tumour biomarkers-Tracin...</td>\n",
       "      <td>30873683</td>\n",
       "      <td>tumour biomarkers-tracing the molecular functi...</td>\n",
       "      <td>in recent year , with the increase in cancer m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30874851&gt;\\nTitle: Pomalidomide, cyclophosp...</td>\n",
       "      <td>30874851</td>\n",
       "      <td>pomalidomide , cyclophosphamide , and dexameth...</td>\n",
       "      <td>pomalidomide dexamethasone is a standard of ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875581&gt;\\nTitle: Aggressive variants of p...</td>\n",
       "      <td>30875581</td>\n",
       "      <td>aggressive variant of prostate cancer - are we...</td>\n",
       "      <td>recently , adoption of novel drug for systemic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875950&gt;\\nTitle: Circulating Tumour Cells...</td>\n",
       "      <td>30875950</td>\n",
       "      <td>circulating tumour cell ( ctc ) , head and nec...</td>\n",
       "      <td>head and neck cancer is the seventh most commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38623902&gt;\\nTitle: [Not Available].\\nAbstra...</td>\n",
       "      <td>38623902</td>\n",
       "      <td>[ not available ] .</td>\n",
       "      <td>effective longitudinal biomarkers that track d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38640937&gt;\\nTitle: Mechanisms and managemen...</td>\n",
       "      <td>38640937</td>\n",
       "      <td>mechanism and management of loss of response t...</td>\n",
       "      <td>we sought to report the effectiveness of infli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38642556&gt;\\nTitle: Modification of coronary...</td>\n",
       "      <td>38642556</td>\n",
       "      <td>modification of coronary artery disease clinic...</td>\n",
       "      <td>the extent to which the relationship between c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38650020&gt;\\nTitle: Meta-analysis of the glo...</td>\n",
       "      <td>38650020</td>\n",
       "      <td>meta-analysis of the global distribution of cl...</td>\n",
       "      <td>cyp2c8 is responsible for the metabolism of 5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38701783&gt;\\nTitle: FLT3L governs the develo...</td>\n",
       "      <td>38701783</td>\n",
       "      <td>flt3l governs the development of partially ove...</td>\n",
       "      <td>fms-related tyrosine kinase 3 ligand ( flt3l )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_path       label  \\\n",
       "0    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "1    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "2    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "3    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "4    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "..                                                 ...         ...   \n",
       "995  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "996  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "997  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "998  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "999  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "\n",
       "                                               content       ids  \\\n",
       "0    <ID:30872385>\\nTitle: Comparison of methodolog...  30872385   \n",
       "1    <ID:30873683>\\nTitle: Tumour biomarkers-Tracin...  30873683   \n",
       "2    <ID:30874851>\\nTitle: Pomalidomide, cyclophosp...  30874851   \n",
       "3    <ID:30875581>\\nTitle: Aggressive variants of p...  30875581   \n",
       "4    <ID:30875950>\\nTitle: Circulating Tumour Cells...  30875950   \n",
       "..                                                 ...       ...   \n",
       "995  <ID:38623902>\\nTitle: [Not Available].\\nAbstra...  38623902   \n",
       "996  <ID:38640937>\\nTitle: Mechanisms and managemen...  38640937   \n",
       "997  <ID:38642556>\\nTitle: Modification of coronary...  38642556   \n",
       "998  <ID:38650020>\\nTitle: Meta-analysis of the glo...  38650020   \n",
       "999  <ID:38701783>\\nTitle: FLT3L governs the develo...  38701783   \n",
       "\n",
       "                                                 title  \\\n",
       "0    comparison of methodology for the detection of...   \n",
       "1    tumour biomarkers-tracing the molecular functi...   \n",
       "2    pomalidomide , cyclophosphamide , and dexameth...   \n",
       "3    aggressive variant of prostate cancer - are we...   \n",
       "4    circulating tumour cell ( ctc ) , head and nec...   \n",
       "..                                                 ...   \n",
       "995                                [ not available ] .   \n",
       "996  mechanism and management of loss of response t...   \n",
       "997  modification of coronary artery disease clinic...   \n",
       "998  meta-analysis of the global distribution of cl...   \n",
       "999  flt3l governs the development of partially ove...   \n",
       "\n",
       "                                              abstract  \n",
       "0    aim : braf v600e detection assist in the diagn...  \n",
       "1    in recent year , with the increase in cancer m...  \n",
       "2    pomalidomide dexamethasone is a standard of ca...  \n",
       "3    recently , adoption of novel drug for systemic...  \n",
       "4    head and neck cancer is the seventh most commo...  \n",
       "..                                                 ...  \n",
       "995  effective longitudinal biomarkers that track d...  \n",
       "996  we sought to report the effectiveness of infli...  \n",
       "997  the extent to which the relationship between c...  \n",
       "998  cyp2c8 is responsible for the metabolism of 5 ...  \n",
       "999  fms-related tyrosine kinase 3 ligand ( flt3l )...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a0f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrityunjay\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Sentence embedding model 'all-MiniLM-L6-v2' loaded successfully.\n",
      "INFO:__main__:SentenceTransformer model using device: cpu\n",
      "ERROR:__main__:cleaned_data.csv not found. Please ensure it exists in the same directory.\n",
      "INFO:__main__:Processed 640/1000 texts...\n",
      "INFO:__main__:Embeddings generated successfully. Shape: (1000, 384)\n",
      "INFO:__main__:Embeddings and labels saved successfully.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# # We will use SentenceTransformer, so no need for AutoModel, AutoModelForCausalLM directly\n",
    "# # from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import logging\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # --- Model Name for Sentence Transformers ---\n",
    "# # These are much smaller and faster on CPU.\n",
    "# # 'all-MiniLM-L6-v2' is a good balance of size/performance.\n",
    "# # 'all-mpnet-base-v2' is slightly larger but often more performant.\n",
    "# SENTENCE_TRANSFORMER_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "# # --------------------------------------------\n",
    "\n",
    "# def load_embedding_model(model_name=SENTENCE_TRANSFORMER_MODEL_NAME):\n",
    "#     \"\"\"\n",
    "#     Load a pre-trained sentence embedding model from sentence-transformers.\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): Name of the pre-trained model (e.g., 'all-MiniLM-L6-v2').\n",
    "\n",
    "#     Returns:\n",
    "#         SentenceTransformer: The loaded model.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # SentenceTransformer handles its own tokenizer and device management internally\n",
    "#         # It automatically uses CPU if CUDA is not available.\n",
    "#         model = SentenceTransformer(model_name)\n",
    "#         logger.info(f\"Sentence embedding model '{model_name}' loaded successfully.\")\n",
    "        \n",
    "#         # Log the actual device being used by the SentenceTransformer model\n",
    "#         # SentenceTransformer wraps a Hugging Face model, so we can check its device\n",
    "#         if hasattr(model, 'device'):\n",
    "#             logger.info(f\"SentenceTransformer model using device: {model.device}\")\n",
    "#         else:\n",
    "#             # Fallback for older versions or different internal structures\n",
    "#             logger.info(\"Could not determine specific device for SentenceTransformer model, assuming CPU.\")\n",
    "\n",
    "#         return model\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while loading the embedding model: {e}\")\n",
    "#         raise\n",
    "\n",
    "# def get_embedding(text, model):\n",
    "#     \"\"\"\n",
    "#     Generate embeddings for a given text using a SentenceTransformer model.\n",
    "\n",
    "#     Args:\n",
    "#         text (str): The text to generate embeddings for.\n",
    "#         model: The SentenceTransformer model to use.\n",
    "\n",
    "#     Returns:\n",
    "#         numpy.ndarray: The embeddings for the text.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # SentenceTransformer's encode method handles tokenization, padding,\n",
    "#         # and device placement internally.\n",
    "#         # It returns a numpy array by default.\n",
    "#         embeddings = model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "#         # Ensure the embedding is always a 2D array (e.g., (1, D) for a single text)\n",
    "#         # model.encode can return 1D for single string or 2D for list of strings\n",
    "#         if embeddings.ndim == 1:\n",
    "#             embeddings = np.expand_dims(embeddings, axis=0)\n",
    "\n",
    "#         return embeddings\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while generating embeddings for text: '{text[:50]}...': {e}\")\n",
    "#         return None\n",
    "\n",
    "# def generate_embeddings(df, model): # Removed tokenizer from args, not needed\n",
    "#     \"\"\"\n",
    "#     Generate embeddings for texts in a DataFrame.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The DataFrame containing the texts.\n",
    "#         model: The embedding model to use.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing the embeddings and labels.\n",
    "#     \"\"\"\n",
    "#     all_embeddings = []\n",
    "#     labels = []\n",
    "    \n",
    "#     try:\n",
    "#         # Combine title and abstract\n",
    "#         df[\"text\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\") # Handle potential NaN values\n",
    "\n",
    "#         if \"label\" not in df.columns:\n",
    "#             logger.error(\"DataFrame does not contain a 'label' column.\")\n",
    "#             return None, None\n",
    "        \n",
    "#         # --- Batch processing for efficiency (SentenceTransformer can do this easily) ---\n",
    "#         # It's more efficient to encode a list of texts at once with SentenceTransformer\n",
    "#         # Adjust batch_size based on your RAM. For 8GB, 32-64 might be a good starting point.\n",
    "#         batch_size = 64\n",
    "        \n",
    "#         texts_to_process = df[\"text\"].tolist()\n",
    "#         df_labels = df[\"label\"].tolist() # Get labels in the same order\n",
    "        \n",
    "#         for i in range(0, len(texts_to_process), batch_size):\n",
    "#             batch_texts = texts_to_process[i:i + batch_size]\n",
    "#             batch_labels = df_labels[i:i + batch_size] # Corresponding labels\n",
    "\n",
    "#             current_batch_embeddings = model.encode(batch_texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "            \n",
    "#             # Ensure embeddings are 2D (batch_size, embedding_dim)\n",
    "#             if current_batch_embeddings.ndim == 1: # Should not happen with batch_size > 1\n",
    "#                  current_batch_embeddings = np.expand_dims(current_batch_embeddings, axis=0)\n",
    "\n",
    "#             for j in range(len(current_batch_embeddings)):\n",
    "#                 all_embeddings.append(np.expand_dims(current_batch_embeddings[j], axis=0)) # Append as (1,D)\n",
    "#                 labels.append(batch_labels[j])\n",
    "\n",
    "#             # Optional: Print progress\n",
    "#             if (i // batch_size + 1) % 10 == 0: # Print every 10 batches\n",
    "#                 logger.info(f\"Processed {i + len(batch_texts)}/{len(texts_to_process)} texts...\")\n",
    "\n",
    "#         if not all_embeddings:\n",
    "#             logger.error(\"No embeddings were generated. Check for issues in get_embedding.\")\n",
    "#             return None, None\n",
    "\n",
    "#         X = np.vstack(all_embeddings)\n",
    "#         y = pd.Series(labels).map({'Cancer': 1, 'Non-Cancer': 0}).values\n",
    "\n",
    "#         logger.info(f\"Embeddings generated successfully. Shape: {X.shape}\")\n",
    "#         return X, y\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while generating embeddings for the DataFrame: {e}\")\n",
    "#         raise\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load embedding model (no separate tokenizer needed for SentenceTransformer)\n",
    "#     model = load_embedding_model()\n",
    "\n",
    "#     try:\n",
    "#         df = pd.read_csv(r\"C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classification\\classification_pipeline\\app\\cleaned_data.csv\")\n",
    "#         logger.info(\"cleaned_data.csv loaded successfully.\")\n",
    "        \n",
    "#         required_cols = [\"title\", \"abstract\", \"label\"]\n",
    "#         if not all(col in df.columns for col in required_cols):\n",
    "#             logger.error(f\"Missing one or more required columns ({required_cols}) in cleaned_data.csv.\")\n",
    "#             exit()\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         logger.error(\"cleaned_data.csv not found. Please ensure it exists in the same directory.\")\n",
    "#         exit()\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while loading cleaned_data.csv: {e}\")\n",
    "#         exit()\n",
    "\n",
    "#     # Generate embeddings\n",
    "#     X, y = generate_embeddings(df, model) \n",
    "\n",
    "#     if X is not None and y is not None:\n",
    "#         np.save(\"embeddings.npy\", X)\n",
    "#         np.save(\"labels.npy\", y)\n",
    "#         logger.info(\"Embeddings and labels saved successfully.\")\n",
    "#     else:\n",
    "#         logger.error(\"Failed to generate embeddings and labels, not saving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70972697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>ids</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30872385&gt;\\nTitle: Comparison of methodolog...</td>\n",
       "      <td>30872385</td>\n",
       "      <td>comparison of methodology for the detection of...</td>\n",
       "      <td>aim : braf v600e detection assist in the diagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30873683&gt;\\nTitle: Tumour biomarkers-Tracin...</td>\n",
       "      <td>30873683</td>\n",
       "      <td>tumour biomarkers-tracing the molecular functi...</td>\n",
       "      <td>in recent year , with the increase in cancer m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30874851&gt;\\nTitle: Pomalidomide, cyclophosp...</td>\n",
       "      <td>30874851</td>\n",
       "      <td>pomalidomide , cyclophosphamide , and dexameth...</td>\n",
       "      <td>pomalidomide dexamethasone is a standard of ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875581&gt;\\nTitle: Aggressive variants of p...</td>\n",
       "      <td>30875581</td>\n",
       "      <td>aggressive variant of prostate cancer - are we...</td>\n",
       "      <td>recently , adoption of novel drug for systemic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>&lt;ID:30875950&gt;\\nTitle: Circulating Tumour Cells...</td>\n",
       "      <td>30875950</td>\n",
       "      <td>circulating tumour cell ( ctc ) , head and nec...</td>\n",
       "      <td>head and neck cancer is the seventh most commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38623902&gt;\\nTitle: [Not Available].\\nAbstra...</td>\n",
       "      <td>38623902</td>\n",
       "      <td>[ not available ] .</td>\n",
       "      <td>effective longitudinal biomarkers that track d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38640937&gt;\\nTitle: Mechanisms and managemen...</td>\n",
       "      <td>38640937</td>\n",
       "      <td>mechanism and management of loss of response t...</td>\n",
       "      <td>we sought to report the effectiveness of infli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38642556&gt;\\nTitle: Modification of coronary...</td>\n",
       "      <td>38642556</td>\n",
       "      <td>modification of coronary artery disease clinic...</td>\n",
       "      <td>the extent to which the relationship between c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38650020&gt;\\nTitle: Meta-analysis of the glo...</td>\n",
       "      <td>38650020</td>\n",
       "      <td>meta-analysis of the global distribution of cl...</td>\n",
       "      <td>cyp2c8 is responsible for the metabolism of 5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...</td>\n",
       "      <td>Non-Cancer</td>\n",
       "      <td>&lt;ID:38701783&gt;\\nTitle: FLT3L governs the develo...</td>\n",
       "      <td>38701783</td>\n",
       "      <td>flt3l governs the development of partially ove...</td>\n",
       "      <td>fms-related tyrosine kinase 3 ligand ( flt3l )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_path       label  \\\n",
       "0    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "1    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "2    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "3    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "4    C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...      Cancer   \n",
       "..                                                 ...         ...   \n",
       "995  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "996  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "997  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "998  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "999  C:\\Users\\Mrityunjay\\Desktop\\lung_cancer_classi...  Non-Cancer   \n",
       "\n",
       "                                               content       ids  \\\n",
       "0    <ID:30872385>\\nTitle: Comparison of methodolog...  30872385   \n",
       "1    <ID:30873683>\\nTitle: Tumour biomarkers-Tracin...  30873683   \n",
       "2    <ID:30874851>\\nTitle: Pomalidomide, cyclophosp...  30874851   \n",
       "3    <ID:30875581>\\nTitle: Aggressive variants of p...  30875581   \n",
       "4    <ID:30875950>\\nTitle: Circulating Tumour Cells...  30875950   \n",
       "..                                                 ...       ...   \n",
       "995  <ID:38623902>\\nTitle: [Not Available].\\nAbstra...  38623902   \n",
       "996  <ID:38640937>\\nTitle: Mechanisms and managemen...  38640937   \n",
       "997  <ID:38642556>\\nTitle: Modification of coronary...  38642556   \n",
       "998  <ID:38650020>\\nTitle: Meta-analysis of the glo...  38650020   \n",
       "999  <ID:38701783>\\nTitle: FLT3L governs the develo...  38701783   \n",
       "\n",
       "                                                 title  \\\n",
       "0    comparison of methodology for the detection of...   \n",
       "1    tumour biomarkers-tracing the molecular functi...   \n",
       "2    pomalidomide , cyclophosphamide , and dexameth...   \n",
       "3    aggressive variant of prostate cancer - are we...   \n",
       "4    circulating tumour cell ( ctc ) , head and nec...   \n",
       "..                                                 ...   \n",
       "995                                [ not available ] .   \n",
       "996  mechanism and management of loss of response t...   \n",
       "997  modification of coronary artery disease clinic...   \n",
       "998  meta-analysis of the global distribution of cl...   \n",
       "999  flt3l governs the development of partially ove...   \n",
       "\n",
       "                                              abstract  \n",
       "0    aim : braf v600e detection assist in the diagn...  \n",
       "1    in recent year , with the increase in cancer m...  \n",
       "2    pomalidomide dexamethasone is a standard of ca...  \n",
       "3    recently , adoption of novel drug for systemic...  \n",
       "4    head and neck cancer is the seventh most commo...  \n",
       "..                                                 ...  \n",
       "995  effective longitudinal biomarkers that track d...  \n",
       "996  we sought to report the effectiveness of infli...  \n",
       "997  the extent to which the relationship between c...  \n",
       "998  cyp2c8 is responsible for the metabolism of 5 ...  \n",
       "999  fms-related tyrosine kinase 3 ligand ( flt3l )...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c6f54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = df[\"title\"] + \" \" + df[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62ba7990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pipeline loaded successfully with model facebook/bart-large-mnli.\n",
      "INFO:__main__:Prediction completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: ['Cancer', 'Non-Cancer']\n",
      "Confidence Scores: [0.5350133776664734, 0.4649865925312042]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_baseline_pipeline(model_name=\"facebook/bart-large-mnli\"):\n",
    "    \"\"\"\n",
    "    Load a zero-shot classification pipeline using a pre-trained model from Hugging Face.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model to use for the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: A Hugging Face pipeline object for zero-shot classification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "        logger.info(f\"Pipeline loaded successfully with model {model_name}.\")\n",
    "        return classifier\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while loading the pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "def predict_baseline(classifier, text, candidate_labels=[\"Cancer\", \"Non-Cancer\"]):\n",
    "    \"\"\"\n",
    "    Predict the class of a given text using a zero-shot classification pipeline.\n",
    "\n",
    "    Args:\n",
    "        classifier: The Hugging Face pipeline object for zero-shot classification.\n",
    "        text (str): The text to classify.\n",
    "        candidate_labels (list): List of candidate labels for classification.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the predicted labels and their corresponding scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels=candidate_labels)\n",
    "        logger.info(\"Prediction completed successfully.\")\n",
    "        return result['labels'], result['scores']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during prediction: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the baseline pipeline\n",
    "    classifier = load_baseline_pipeline()\n",
    "\n",
    "    # from data_loader import load_and_parse_data\n",
    "    # input_data = load_and_parse_data()\n",
    "    # #print(input_data)\n",
    "    # Example text to classify\n",
    "    #example_text = [i for i in input_data[\"abstract\"]]\n",
    "    #print(example_text)\n",
    "    #example_text = \"This study explores the genetic markers associated with lung cancer.\"\n",
    "\n",
    "    # Make a prediction\n",
    "    labels, scores = predict_baseline(classifier, example_text)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Predicted Labels:\", labels)\n",
    "    print(\"Confidence Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cdf9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Tokenizer loaded successfully for model distilbert-base-uncased.\n",
      "Map: 100%|██████████| 1000/1000 [00:01<00:00, 557.32 examples/s]\n",
      "INFO:__main__:Dataset prepared and tokenized successfully.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:LoRA model configured successfully for model distilbert-base-uncased.\n",
      "C:\\Users\\Mrityunjay\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/189 [00:00<?, ?it/s]ERROR:__main__:An error occurred during model training: too many dimensions 'str'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 160\u001b[0m\n\u001b[0;32m    157\u001b[0m lora_model \u001b[38;5;241m=\u001b[39m get_lora_model()\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 140\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, tokenized_dataset, output_dir)\u001b[0m\n\u001b[0;32m    120\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m    121\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m    122\u001b[0m         evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    132\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    133\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    134\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    138\u001b[0m     )\n\u001b[1;32m--> 140\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2425\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2426\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2429\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5045\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5047\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\data_loader.py:567\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\data\\data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\data\\data_collator.py:141\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    139\u001b[0m     label \u001b[38;5;241m=\u001b[39m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    140\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[1;32m--> 141\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m first \u001b[38;5;129;01mand\u001b[39;00m first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_tokenizer(model_name=\"distilbert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Load a tokenizer for a given model name.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model.\n",
    "\n",
    "    Returns:\n",
    "        AutoTokenizer: The tokenizer for the specified model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        logger.info(f\"Tokenizer loaded successfully for model {model_name}.\")\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while loading the tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_len=512):\n",
    "    \"\"\"\n",
    "    Tokenize the examples using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Dictionary containing the text data.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        max_len (int): Maximum length for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized examples.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "\n",
    "def prepare_dataset(X, y, tokenizer):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for training.\n",
    "\n",
    "    Args:\n",
    "        X (list): List of text samples.\n",
    "        y (list): List of labels.\n",
    "        tokenizer: The tokenizer to use.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Tokenized dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = Dataset.from_dict({\"text\": X, \"label\": y})\n",
    "        tokenized_dataset = dataset.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "        logger.info(\"Dataset prepared and tokenized successfully.\")\n",
    "        return tokenized_dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while preparing the dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_lora_model(model_name=\"distilbert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Load a model and apply LoRA configuration.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model.\n",
    "\n",
    "    Returns:\n",
    "        PeftModel: The model with LoRA configuration.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "        config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            target_modules=[\"q_lin\", \"v_lin\"] if \"distilbert\" in model_name else [\"query\", \"value\"]\n",
    "        )\n",
    "\n",
    "        lora_model = get_peft_model(model, config)\n",
    "        logger.info(f\"LoRA model configured successfully for model {model_name}.\")\n",
    "        return lora_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while configuring the LoRA model: {e}\")\n",
    "        raise\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple of predictions and label ids.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy': accuracy, 'f1': f1}\n",
    "\n",
    "def train_model(model, tokenized_dataset, output_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Train the model using the provided dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        tokenized_dataset (Dataset): The dataset to use for training.\n",
    "        output_dir (str): Directory to save the model outputs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            eval_dataset=tokenized_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        logger.info(\"Model training completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during model training: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    X = example_text\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    # Load tokenizer and prepare dataset\n",
    "    tokenizer = get_tokenizer()\n",
    "    tokenized_dataset = prepare_dataset(X, y, tokenizer)\n",
    "\n",
    "    # Load and configure LoRA model\n",
    "    lora_model = get_lora_model()\n",
    "\n",
    "    # Train the model\n",
    "    train_model(lora_model, tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a9e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame length: 1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Tokenizer loaded successfully for model distilbert-base-uncased.\n",
      "Map: 100%|██████████| 1008/1008 [00:00<00:00, 2969.51 examples/s]\n",
      "INFO:__main__:Dataset prepared and tokenized successfully.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:LoRA model configured successfully for model distilbert-base-uncased.\n",
      "  0%|          | 0/189 [16:45<?, ?it/s]\n",
      "  7%|▋         | 10/153 [06:03<1:31:25, 38.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6913, 'grad_norm': 1.7869631052017212, 'learning_rate': 1.869281045751634e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 20/153 [12:05<1:37:35, 44.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6824, 'grad_norm': 1.0087590217590332, 'learning_rate': 1.738562091503268e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 30/153 [18:00<1:07:18, 32.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6849, 'grad_norm': 0.8809709548950195, 'learning_rate': 1.607843137254902e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 40/153 [23:28<1:02:11, 33.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6843, 'grad_norm': 1.2216254472732544, 'learning_rate': 1.4771241830065362e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 50/153 [29:38<1:03:34, 37.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.667, 'grad_norm': 0.9278779625892639, 'learning_rate': 1.3464052287581701e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|███▎      | 51/153 [31:52<49:15, 28.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6526228785514832, 'eval_accuracy': 0.8415841584158416, 'eval_f1': 0.835845517247001, 'eval_runtime': 123.779, 'eval_samples_per_second': 1.632, 'eval_steps_per_second': 0.105, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 60/153 [37:22<58:44, 37.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6604, 'grad_norm': 1.3572663068771362, 'learning_rate': 1.215686274509804e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 70/153 [42:57<43:25, 31.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.656, 'grad_norm': 0.9571828246116638, 'learning_rate': 1.0849673202614379e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 80/153 [48:01<37:05, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6413, 'grad_norm': 1.3479958772659302, 'learning_rate': 9.54248366013072e-06, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 90/153 [53:10<32:29, 30.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6428, 'grad_norm': 1.1791025400161743, 'learning_rate': 8.23529411764706e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 100/153 [58:28<28:13, 31.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6319, 'grad_norm': 1.5587539672851562, 'learning_rate': 6.928104575163399e-06, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 102/153 [1:00:53<21:02, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6136587262153625, 'eval_accuracy': 0.9158415841584159, 'eval_f1': 0.9156661638355404, 'eval_runtime': 104.7999, 'eval_samples_per_second': 1.927, 'eval_steps_per_second': 0.124, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 110/153 [1:05:26<27:11, 37.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6286, 'grad_norm': 1.657580018043518, 'learning_rate': 5.620915032679739e-06, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 120/153 [1:11:48<18:28, 33.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6231, 'grad_norm': 1.4232587814331055, 'learning_rate': 4.313725490196079e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 130/153 [1:16:48<11:40, 30.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6079, 'grad_norm': 1.3350950479507446, 'learning_rate': 3.0065359477124182e-06, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 140/153 [1:21:49<06:30, 30.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6053, 'grad_norm': 2.006324291229248, 'learning_rate': 1.6993464052287585e-06, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 150/153 [1:26:55<01:30, 30.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6087, 'grad_norm': 2.0472679138183594, 'learning_rate': 3.921568627450981e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 153/153 [1:29:44<00:00, 23.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5904716849327087, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 100.5648, 'eval_samples_per_second': 2.009, 'eval_steps_per_second': 0.129, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [1:29:45<00:00, 35.20s/it]\n",
      "INFO:__main__:Model training completed successfully.\n",
      "INFO:__main__:Evaluating the trained model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5385.0817, 'train_samples_per_second': 0.449, 'train_steps_per_second': 0.028, 'train_loss': 0.646938322416318, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:56<00:00,  8.98s/it]\n",
      "INFO:__main__:Evaluation results: {'eval_loss': 0.5904716849327087, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 126.2084, 'eval_samples_per_second': 1.601, 'eval_steps_per_second': 0.103, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd # Import pandas\n",
    "import torch # Import torch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_tokenizer(model_name=\"distilbert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Load a tokenizer for a given model name.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model.\n",
    "\n",
    "    Returns:\n",
    "        AutoTokenizer: The tokenizer for the specified model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        logger.info(f\"Tokenizer loaded successfully for model {model_name}.\")\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while loading the tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_len=512):\n",
    "    \"\"\"\n",
    "    Tokenize the examples using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Dictionary containing the text data.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        max_len (int): Maximum length for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized examples.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "\n",
    "def prepare_dataset(X, y, tokenizer):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for training.\n",
    "\n",
    "    Args:\n",
    "        X (list): List of text samples.\n",
    "        y (list): List of labels.\n",
    "        tokenizer: The tokenizer to use.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Tokenized dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(y, list):\n",
    "            y = y.tolist()\n",
    "        y = [int(label) for label in y]\n",
    "\n",
    "        if not isinstance(X, list):\n",
    "            X = X.tolist()\n",
    "\n",
    "        dataset = Dataset.from_dict({\"text\": X, \"label\": y})\n",
    "        tokenized_dataset = dataset.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "        tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "        logger.info(\"Dataset prepared and tokenized successfully.\")\n",
    "        return tokenized_dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while preparing the dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_lora_model(model_name=\"distilbert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Load a model and apply LoRA configuration.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model.\n",
    "\n",
    "    Returns:\n",
    "        PeftModel: The model with LoRA configuration.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "        config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            target_modules=[\"q_lin\", \"v_lin\"] if \"distilbert\" in model_name else [\"query\", \"value\"]\n",
    "        )\n",
    "\n",
    "        lora_model = get_peft_model(model, config)\n",
    "        logger.info(f\"LoRA model configured successfully for model {model_name}.\")\n",
    "        return lora_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while configuring the LoRA model: {e}\")\n",
    "        raise\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple of predictions and label ids.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy': accuracy, 'f1': f1}\n",
    "\n",
    "def train_model(model, tokenized_dataset, output_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Train the model using the provided dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        tokenized_dataset (Dataset): The dataset to use for training.\n",
    "        output_dir (str): Directory to save the model outputs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        train_val_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        train_dataset = train_val_split[\"train\"]\n",
    "        eval_dataset = train_val_split[\"test\"]\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",  # Evaluation per epoch\n",
    "            save_strategy=\"epoch\",        \n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            save_total_limit=1,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            # If you were saving by steps, you'd also set save_steps and eval_steps:\n",
    "            # save_steps=500,\n",
    "            # eval_steps=500,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        logger.info(\"Model training completed successfully.\")\n",
    "\n",
    "        logger.info(\"Evaluating the trained model...\")\n",
    "        eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "        logger.info(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during model training: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage (remains the same as previous correct version)\n",
    "if __name__ == \"__main__\":\n",
    "    data = {\n",
    "        'text': [\"This is a positive sentence.\", \"This is a negative sentence.\", \"Another positive.\", \"Another negative.\", \"Positive example five.\", \"Negative example six.\",\n",
    "                 \"I love this product!\", \"This is terrible.\", \"Fantastic movie!\", \"Worst experience ever.\", \"So happy with the results.\", \"Disappointing outcome.\"],\n",
    "        'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "    }\n",
    "    long_data = {\n",
    "        'text': data['text'] * 84,\n",
    "        'label': data['label'] * 84\n",
    "    }\n",
    "    df = pd.DataFrame(long_data)\n",
    "    print(f\"DataFrame length: {len(df)}\")\n",
    "\n",
    "    X = df[\"text\"]\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    tokenizer = get_tokenizer()\n",
    "    tokenized_dataset = prepare_dataset(X, y, tokenizer)\n",
    "\n",
    "    lora_model = get_lora_model()\n",
    "\n",
    "    train_model(lora_model, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ee2fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_lora_tokenizer\\\\tokenizer_config.json',\n",
       " 'my_lora_tokenizer\\\\special_tokens_map.json',\n",
       " 'my_lora_tokenizer\\\\vocab.txt',\n",
       " 'my_lora_tokenizer\\\\added_tokens.json',\n",
       " 'my_lora_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In your training script's train_model function, after trainer.train():\n",
    "# To save LoRA adapters:\n",
    "model.save_pretrained(\"my_lora_model_adapters\") # This saves the LoRA parts\n",
    "tokenizer.save_pretrained(\"my_lora_tokenizer\") # Save the tokenizer too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eabfffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting to create dummy model and tokenizer files for testing...\n",
      "INFO:__main__:Dummy tokenizer saved to ./my_lora_tokenizer\n",
      "INFO:__main__:Dummy LoRA adapter path created at ./my_lora_model_adapters\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Base model 'distilbert-base-uncased' loaded successfully.\n",
      "ERROR:__main__:An error occurred while loading the model or tokenizer: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './my_lora_model_adapters'.\n",
      "ERROR:__main__:Failed to load model and tokenizer. Please ensure `BASE_MODEL_NAME`, `LORA_ADAPTERS_PATH`, and `TOKENIZER_SAVE_PATH` are correct and point to existing files/directories.\n",
      "INFO:__main__:Data preprocessing completed successfully.\n",
      "ERROR:__main__:An error occurred during prediction: index -1 is out of bounds for dimension 1 with size 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m inputs \u001b[38;5;241m=\u001b[39m preprocess_data(texts, tokenizer)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Postprocess predictions\u001b[39;00m\n\u001b[0;32m    159\u001b[0m predictions \u001b[38;5;241m=\u001b[39m postprocess_predictions(logits)\n",
      "Cell \u001b[1;32mIn[30], line 84\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, inputs)\u001b[0m\n\u001b[0;32m     82\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 84\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     85\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1785\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(torch\u001b[38;5;241m.\u001b[39munique_consecutive(eos_mask\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m))) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll examples must have the same number of <eos> tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1785\u001b[0m sentence_representation \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43meos_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1788\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head(sentence_representation)\n\u001b[0;32m   1790\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "# predictor.py\n",
    "import logging\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel # Import PeftModel for loading LoRA adapters\n",
    "import pandas as pd # pandas is imported but not used in this specific script's functions directly\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model_and_tokenizer(base_model_name, lora_adapter_path, tokenizer_path):\n",
    "    \"\"\"\n",
    "    Load a trained base model and then apply LoRA adapters,\n",
    "    along with the tokenizer.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): Name of the original base model (e.g., \"distilbert-base-uncased\").\n",
    "        lora_adapter_path (str): Path to the saved LoRA adapter weights.\n",
    "        tokenizer_path (str): Path to the saved tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The loaded model (with LoRA applied) and tokenizer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load the base model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2) # Ensure num_labels matches your task\n",
    "        logger.info(f\"Base model '{base_model_name}' loaded successfully.\")\n",
    "\n",
    "        # 2. Load the LoRA adapters onto the base model\n",
    "        model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "        logger.info(f\"LoRA adapters loaded from '{lora_adapter_path}' and applied to the model.\")\n",
    "\n",
    "        # 3. Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        logger.info(f\"Tokenizer loaded successfully from '{tokenizer_path}'.\")\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while loading the model or tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Preprocess input texts by tokenizing them.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of texts to preprocess.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        max_length (int): Maximum length for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized inputs (PyTorch tensors).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure texts is always a list for consistent behavior\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "\n",
    "        inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        logger.info(\"Data preprocessing completed successfully.\")\n",
    "        return inputs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def predict(model, inputs):\n",
    "    \"\"\"\n",
    "    Make predictions using the loaded model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to use for predictions.\n",
    "        inputs (dict): Tokenized input data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicted logits.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure model is in evaluation mode (already done in load_model_and_tokenizer, but good practice)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logger.info(\"Predictions completed successfully.\")\n",
    "        return outputs.logits\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during prediction: {e}\")\n",
    "        raise\n",
    "\n",
    "def postprocess_predictions(logits):\n",
    "    \"\"\"\n",
    "    Postprocess prediction logits to get predicted class labels.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Prediction logits from the model.\n",
    "\n",
    "    Returns:\n",
    "        list: Predicted class labels (integers, e.g., 0 or 1).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictions = torch.argmax(logits, dim=1).tolist()\n",
    "        logger.info(\"Postprocessing of predictions completed successfully.\")\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during postprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths where you saved your fine-tuned model's adapters and tokenizer\n",
    "    # IMPORTANT: Replace these with the actual paths from your training script's output\n",
    "    # For example, if you saved them to \"./results/my_lora_model_adapters\" and \"./results/my_lora_tokenizer\"\n",
    "    BASE_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "    LORA_ADAPTERS_PATH = \"./my_lora_model_adapters\" # <--- IMPORTANT: This needs to be the path where you save your LoRA adapters!\n",
    "    TOKENIZER_SAVE_PATH = \"./my_lora_tokenizer\"    # <--- IMPORTANT: This needs to be the path where you save your tokenizer!\n",
    "\n",
    "    # Dummy save for demonstration - in real usage, these would be generated by your training script\n",
    "    # This block is for testing this `predictor.py` script independently.\n",
    "    # In a full workflow, your training script would save these.\n",
    "    try:\n",
    "        logger.info(\"Attempting to create dummy model and tokenizer files for testing...\")\n",
    "        # Create a dummy tokenizer in the specified path\n",
    "        dummy_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "        dummy_tokenizer.save_pretrained(TOKENIZER_SAVE_PATH)\n",
    "\n",
    "        # Create a dummy LoRA adapter path. In a real scenario, this would be from trainer.model.save_pretrained\n",
    "        # We need to ensure a directory exists for `PeftModel.from_pretrained`\n",
    "        import os\n",
    "        os.makedirs(LORA_ADAPTERS_PATH, exist_ok=True)\n",
    "        # Create a minimal config.json if not present, though PeftModel might not strictly require it for adapters\n",
    "        # This is just to make the directory look like a saved PEFT model for a basic test\n",
    "        with open(os.path.join(LORA_ADAPTERS_PATH, \"adapter_config.json\"), \"w\") as f:\n",
    "            f.write('{\"peft_type\": \"LORA\", \"task_type\": \"SEQ_CLS\", \"r\": 8, \"lora_alpha\": 32, \"target_modules\": [\"q_lin\", \"v_lin\"]}')\n",
    "        logger.info(f\"Dummy tokenizer saved to {TOKENIZER_SAVE_PATH}\")\n",
    "        logger.info(f\"Dummy LoRA adapter path created at {LORA_ADAPTERS_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not create dummy model/tokenizer for testing: {e}. Please ensure you have trained and saved a model.\")\n",
    "        logger.warning(\"Continuing with the assumption that your paths are correctly set and models exist.\")\n",
    "\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    try:\n",
    "        model, tokenizer = load_model_and_tokenizer(BASE_MODEL_NAME, LORA_ADAPTERS_PATH, TOKENIZER_SAVE_PATH)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to load model and tokenizer. Please ensure `BASE_MODEL_NAME`, `LORA_ADAPTERS_PATH`, and `TOKENIZER_SAVE_PATH` are correct and point to existing files/directories.\")\n",
    "        exit(1) # Exit if loading fails\n",
    "\n",
    "    # Example texts to predict\n",
    "    texts = [\"This patient shows symptoms consistent with early-stage cancer.\", \"The report indicates no abnormalities, suggesting a healthy outcome.\"]\n",
    "\n",
    "    # Preprocess data\n",
    "    inputs = preprocess_data(texts, tokenizer)\n",
    "\n",
    "    # Make predictions\n",
    "    logits = predict(model, inputs)\n",
    "\n",
    "    # Postprocess predictions\n",
    "    predictions = postprocess_predictions(logits)\n",
    "\n",
    "    # Map numerical predictions to human-readable labels if desired\n",
    "    label_map = {0: \"Not Cancer\", 1: \"Cancer\"} # Adjust these based on your actual label mapping during training\n",
    "    human_readable_predictions = [label_map[p] for p in predictions]\n",
    "\n",
    "    # Print predictions\n",
    "    print(\"Input Texts:\", texts)\n",
    "    print(\"Numerical Predictions:\", predictions)\n",
    "    print(\"Human-readable Predictions:\", human_readable_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5eae8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Attempting to create dummy model and tokenizer files for testing...\n",
      "INFO:__main__:Dummy tokenizer saved to ./my_lora_tokenizer\n",
      "INFO:__main__:Dummy LoRA adapter path created at ./my_lora_model_adapters\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Base model 'distilbert-base-uncased' loaded successfully.\n",
      "C:\\Users\\Mrityunjay\\AppData\\Roaming\\Python\\Python310\\site-packages\\peft\\config.py:165: UserWarning: Unexpected keyword arguments ['fan_in_feature_names', 'fan_out_feature_names', 'with_prompt_embedding'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "ERROR:__main__:An error occurred while loading the model or tokenizer: 'base_model.model.pre_classifier.weight'\n",
      "ERROR:__main__:Failed to load model and tokenizer: 'base_model.model.pre_classifier.weight'. Please ensure `BASE_MODEL_NAME`, `LORA_ADAPTERS_PATH`, and `TOKENIZER_SAVE_PATH` are correct and point to existing files/directories.\n",
      "INFO:__main__:\n",
      "--- Predicting with normal texts ---\n",
      "INFO:__main__:Data preprocessing completed successfully.\n",
      "ERROR:__main__:An error occurred during prediction: index -1 is out of bounds for dimension 1 with size 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 194\u001b[0m\n\u001b[0;32m    192\u001b[0m inputs_normal \u001b[38;5;241m=\u001b[39m preprocess_data(texts_normal, tokenizer)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_normal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m     logits_normal \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_normal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logits_normal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m         predictions_normal \u001b[38;5;241m=\u001b[39m postprocess_predictions(logits_normal)\n",
      "Cell \u001b[1;32mIn[31], line 100\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, inputs)\u001b[0m\n\u001b[0;32m     98\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 100\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    101\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1785\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(torch\u001b[38;5;241m.\u001b[39munique_consecutive(eos_mask\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m))) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll examples must have the same number of <eos> tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1785\u001b[0m sentence_representation \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43meos_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1788\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head(sentence_representation)\n\u001b[0;32m   1790\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "# predictor.py\n",
    "import logging\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel # Import PeftModel for loading LoRA adapters\n",
    "import pandas as pd # pandas is imported but not used in this specific script's functions directly\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model_and_tokenizer(base_model_name, lora_adapter_path, tokenizer_path):\n",
    "    \"\"\"\n",
    "    Load a trained base model and then apply LoRA adapters,\n",
    "    along with the tokenizer.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): Name of the original base model (e.g., \"distilbert-base-uncased\").\n",
    "        lora_adapter_path (str): Path to the saved LoRA adapter weights.\n",
    "        tokenizer_path (str): Path to the saved tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The loaded model (with LoRA applied) and tokenizer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load the base model\n",
    "        # Ensure num_labels matches your actual label range (0, 1 for binary classification)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n",
    "        logger.info(f\"Base model '{base_model_name}' loaded successfully.\")\n",
    "\n",
    "        # 2. Load the LoRA adapters onto the base model\n",
    "        model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "        logger.info(f\"LoRA adapters loaded from '{lora_adapter_path}' and applied to the model.\")\n",
    "\n",
    "        # 3. Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        logger.info(f\"Tokenizer loaded successfully from '{tokenizer_path}'.\")\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while loading the model or tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Preprocess input texts by tokenizing them.\n",
    "    Includes robustness checks for empty inputs.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of texts to preprocess.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        max_length (int): Maximum length for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized inputs (PyTorch tensors), or None if no valid inputs.\n",
    "    \"\"\"\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "\n",
    "    # Filter out empty strings from the input texts\n",
    "    non_empty_texts = [text for text in texts if text and text.strip()]\n",
    "    if not non_empty_texts:\n",
    "        logger.warning(\"All input texts are empty or contain only whitespace. Returning None for inputs.\")\n",
    "        return None # No valid texts to process\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(non_empty_texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        logger.info(\"Data preprocessing completed successfully.\")\n",
    "\n",
    "        # Crucial check: Ensure input_ids are not empty\n",
    "        if inputs is None or \"input_ids\" not in inputs or inputs[\"input_ids\"].numel() == 0:\n",
    "            logger.error(\"Tokenization resulted in empty input_ids. This can happen with very short or empty texts after truncation/padding.\")\n",
    "            return None # Return None if inputs are effectively empty\n",
    "\n",
    "        return inputs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during data preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def predict(model, inputs):\n",
    "    \"\"\"\n",
    "    Make predictions using the loaded model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to use for predictions.\n",
    "        inputs (dict): Tokenized input data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicted logits, or None if inputs are invalid.\n",
    "    \"\"\"\n",
    "    if inputs is None or not inputs.get(\"input_ids\").numel(): # Check if inputs are None or effectively empty\n",
    "        logger.error(\"Prediction inputs are empty or invalid. Skipping prediction.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logger.info(\"Predictions completed successfully.\")\n",
    "        return outputs.logits\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during prediction: {e}\")\n",
    "        raise\n",
    "\n",
    "def postprocess_predictions(logits):\n",
    "    \"\"\"\n",
    "    Postprocess prediction logits to get predicted class labels.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Prediction logits from the model.\n",
    "\n",
    "    Returns:\n",
    "        list: Predicted class labels (integers, e.g., 0 or 1), or empty list if logits are None.\n",
    "    \"\"\"\n",
    "    if logits is None:\n",
    "        logger.warning(\"No logits provided for postprocessing. Returning empty list.\")\n",
    "        return []\n",
    "    try:\n",
    "        predictions = torch.argmax(logits, dim=1).tolist()\n",
    "        logger.info(\"Postprocessing of predictions completed successfully.\")\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during postprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths where you saved your fine-tuned model's adapters and tokenizer\n",
    "    BASE_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "    # Placeholder paths - REPLACE WITH YOUR ACTUAL SAVED MODEL/TOKENIZER PATHS!\n",
    "    LORA_ADAPTERS_PATH = \"./my_lora_model_adapters\"\n",
    "    TOKENIZER_SAVE_PATH = \"./my_lora_tokenizer\"\n",
    "\n",
    "    # --- Dummy save for demonstration (in a real scenario, your training script would do this) ---\n",
    "    logger.info(\"Attempting to create dummy model and tokenizer files for testing...\")\n",
    "    try:\n",
    "        # Create a dummy tokenizer in the specified path\n",
    "        dummy_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "        dummy_tokenizer.save_pretrained(TOKENIZER_SAVE_PATH)\n",
    "\n",
    "        # Create a minimal adapter_config.json to make the path loadable by PeftModel\n",
    "        import os\n",
    "        os.makedirs(LORA_ADAPTERS_PATH, exist_ok=True)\n",
    "        adapter_config_content = {\n",
    "            \"peft_type\": \"LORA\",\n",
    "            \"task_type\": \"SEQ_CLS\",\n",
    "            \"r\": 8,\n",
    "            \"lora_alpha\": 32,\n",
    "            \"lora_dropout\": 0.1,\n",
    "            \"bias\": \"none\",\n",
    "            \"target_modules\": [\"q_lin\", \"v_lin\"], # Must match what you used in LoraConfig\n",
    "            \"fan_in_feature_names\": [],\n",
    "            \"fan_out_feature_names\": [],\n",
    "            \"modules_to_save\": None,\n",
    "            \"init_lora_weights\": True,\n",
    "            \"revision\": None,\n",
    "            \"with_prompt_embedding\": False\n",
    "        }\n",
    "        import json\n",
    "        with open(os.path.join(LORA_ADAPTERS_PATH, \"adapter_config.json\"), \"w\") as f:\n",
    "            json.dump(adapter_config_content, f)\n",
    "\n",
    "        # Also create a dummy pytorch_model.bin or adapter_model.bin to make PeftModel happy\n",
    "        # A truly empty file might cause issues, so let's create a minimal valid tensor file\n",
    "        dummy_state_dict = {\"dummy_key\": torch.empty(0)} # Needs at least one tensor\n",
    "        torch.save(dummy_state_dict, os.path.join(LORA_ADAPTERS_PATH, \"adapter_model.bin\"))\n",
    "\n",
    "        logger.info(f\"Dummy tokenizer saved to {TOKENIZER_SAVE_PATH}\")\n",
    "        logger.info(f\"Dummy LoRA adapter path created at {LORA_ADAPTERS_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not create dummy model/tokenizer for testing: {e}. Please ensure you have trained and saved a model.\")\n",
    "        logger.warning(\"Continuing with the assumption that your paths are correctly set and models exist.\")\n",
    "    # --- End of dummy save block ---\n",
    "\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    try:\n",
    "        model, tokenizer = load_model_and_tokenizer(BASE_MODEL_NAME, LORA_ADAPTERS_PATH, TOKENIZER_SAVE_PATH)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model and tokenizer: {e}. Please ensure `BASE_MODEL_NAME`, `LORA_ADAPTERS_PATH`, and `TOKENIZER_SAVE_PATH` are correct and point to existing files/directories.\")\n",
    "        exit(1) # Exit if loading fails\n",
    "\n",
    "    # Example texts to predict\n",
    "    # Test with normal texts\n",
    "    texts_normal = [\"This patient shows symptoms consistent with early-stage cancer.\", \"The report indicates no abnormalities, suggesting a healthy outcome.\"]\n",
    "    # Test with an empty string or string with only whitespace\n",
    "    texts_with_empty = [\"Valid sentence.\", \"\", \"   \", \"Another valid one.\"]\n",
    "\n",
    "    logger.info(\"\\n--- Predicting with normal texts ---\")\n",
    "    inputs_normal = preprocess_data(texts_normal, tokenizer)\n",
    "    if inputs_normal is not None:\n",
    "        logits_normal = predict(model, inputs_normal)\n",
    "        if logits_normal is not None:\n",
    "            predictions_normal = postprocess_predictions(logits_normal)\n",
    "            label_map = {0: \"Not Cancer\", 1: \"Cancer\"}\n",
    "            human_readable_predictions_normal = [label_map[p] for p in predictions_normal]\n",
    "            print(\"Input Texts (Normal):\", texts_normal)\n",
    "            print(\"Numerical Predictions (Normal):\", predictions_normal)\n",
    "            print(\"Human-readable Predictions (Normal):\", human_readable_predictions_normal)\n",
    "    else:\n",
    "        print(\"Skipped prediction for normal texts due to invalid inputs.\")\n",
    "\n",
    "    logger.info(\"\\n--- Predicting with texts containing empty/whitespace-only strings ---\")\n",
    "    inputs_empty = preprocess_data(texts_with_empty, tokenizer)\n",
    "    if inputs_empty is not None:\n",
    "        logits_empty = predict(model, inputs_empty)\n",
    "        if logits_empty is not None:\n",
    "            predictions_empty = postprocess_predictions(logits_empty)\n",
    "            label_map = {0: \"Not Cancer\", 1: \"Cancer\"}\n",
    "            human_readable_predictions_empty = [label_map[p] for p in predictions_empty]\n",
    "            print(\"Input Texts (With Empty):\", texts_with_empty)\n",
    "            print(\"Numerical Predictions (With Empty):\", predictions_empty)\n",
    "            print(\"Human-readable Predictions (With Empty, only valid ones):\", human_readable_predictions_empty)\n",
    "    else:\n",
    "        print(\"Skipped prediction for texts with empty strings due to invalid inputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623747f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
